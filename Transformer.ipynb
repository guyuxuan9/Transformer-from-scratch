{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Some links\n",
        "- [Code reference](https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb)\n",
        "- [Original Paper -- Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)"
      ],
      "metadata": {
        "id": "0gsl1Vc4RU5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import copy"
      ],
      "metadata": {
        "id": "e04fVIXVR80o"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional encoding\n",
        "The full sentence is passed to the encoder. From the encoder's view, these are just embeddings without any order. Therefore, positional encoding is used to tell the encoder the original position of each word in a sentence.\n",
        "\n",
        "![image](https://github.com/guyuxuan9/Transformer-from-scratch/assets/58468284/c218d9f1-49b3-4ad2-ba0a-2b4fe1625503)\n",
        "\n",
        "\n",
        "**Original paper**:\n",
        "\n",
        "![image](https://github.com/guyuxuan9/Transformer-from-scratch/assets/58468284/53fd61b6-a49f-424c-a0cb-9f8b5e738f23)\n",
        "\n",
        "**Code implementation** (replace $(10k)^{power}$ with $10k e^{power}$, prob easier computation):\n",
        "\n",
        "\n",
        "$ div\\_term = e^{-2i \\frac{log(10k)}{d_{model} } } = \\frac{1} { 10k \\cdot e^{\\frac{2i}{d_{model}}}}$,\n",
        "\n",
        "$even\\_pos = sin(position \\times div\\_term)$, $odd\\_pos = cos(position \\times div\\_term)$"
      ],
      "metadata": {
        "id": "oT-YWotRs8Z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) # 0 to end, step size = 2\n",
        "        pe[:, 1::2] = torch.cos(position * div_term) # 1 to end, step size = 2\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0)) # store model state, Device Synchronization, Persistence, Serializing and Loading\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "CxkrvxXVQ6li"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html): base class for all neural network modules. Every model should be subclass of this:\n",
        "\n",
        "```\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        ...\n",
        "    def forward(self, x):\n",
        "        ...\n",
        "        # return output\n",
        "```\n",
        "- model = Model(). No need to call forward like model.forward().\n",
        "- model(x) gives the output\n",
        "\n",
        "# MultiHeadAttention\n",
        "**Intuition**: Each head captures different relationships between words.\n",
        "\n",
        "_NB_: The following code and explanation implements the multihead attention a bit differently from that in the original paper. $Q,K,V \\in \\mathbb{R}^{seq, d_{model}}$\n",
        "\n",
        "Original paper:\n",
        "-  $W_i^Q \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}, \\quad W_i^K \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}, \\quad W_i^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}, \\quad \\text{and} \\quad W^O \\in \\mathbb{R}^{h \\cdot d_v \\times d_{\\text{model}}}$\n",
        "- These are weight matrix for each head\n",
        "\n",
        "Code below:\n",
        "- $W_i^Q \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{model} }, \\quad W_i^K \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{model} }, \\quad W_i^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{model} }, \\quad W^O \\in \\mathbb{R}^{h \\cdot d_v \\times d_{\\text{model}}}$\n",
        "- Only one big weight matrix for $Q,K,V$.\n",
        "- Split the heads after linear transformation.\n",
        "\n",
        "![image](https://github.com/guyuxuan9/UROP_robotic_arm/assets/58468284/5e12311f-dee3-4d09-9dce-90e81c93458c)"
      ],
      "metadata": {
        "id": "JtZPIk3uRyGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Layer for $Q$, $K$ and $V$\n",
        "\n",
        "$Q_{original} = \\begin{bmatrix}\n",
        "    q_{1,1} & q_{1,2} & \\dots & q_{1,d_{model}} \\\\\n",
        "    q_{2,1} & q_{2,2} & \\dots & q_{2,d_{model}} \\\\\n",
        "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "    q_{\\text{seq},1} & q_{\\text{seq},2} & \\dots & q_{\\text{seq},d_{model}} \\\\\n",
        "\\end{bmatrix}$,    $K_{original} = \\begin{bmatrix}\n",
        "    k_{1,1} & k_{1,2} & \\dots & k_{1,d_{model}} \\\\\n",
        "    k_{2,1} & k_{2,2} & \\dots & k_{2,d_{model}} \\\\\n",
        "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "    k_{\\text{seq},1} & k_{\\text{seq},2} & \\dots & k_{\\text{seq},d_{model}} \\\\\n",
        "\\end{bmatrix}$\n",
        "\n",
        "The weight matrix $W_Q$ and $W_K$ have the learnable parameters. They are described by the _nn.Linear_ function. They all have dimension ($d_{model}, d_{model}$).\n",
        "\n",
        "$W_Q = \\begin{bmatrix}\n",
        "\\vdots & \\vdots & \\dots & \\vdots \\\\\n",
        "w_1^Q & w_2^Q & \\dots & w_{d_{model}}^Q \\\\\n",
        "\\vdots & \\vdots & \\dots & \\vdots\n",
        "\\end{bmatrix}$ $W_K = \\begin{bmatrix}\n",
        "\\vdots & \\vdots & \\dots & \\vdots \\\\\n",
        "w_1^K & w_2^K & \\dots & w_{d_{model}}^K \\\\\n",
        "\\vdots & \\vdots & \\dots & \\vdots\n",
        "\\end{bmatrix}$\n",
        "\n",
        "By multiplying the original embeddings with the learnable weights, the network can learn more patterns, increasing the expressive power than self-attention.\n",
        "\n",
        "$Q = Q_{original}W_Q = \\begin{bmatrix}\n",
        "    q_{1,1}w_{1,1}^Q & q_{1,2}w_{1,2}^Q & \\dots & q_{1,d_{model}}w_{1,d_{model}}^Q \\\\\n",
        "    q_{2,1}w_{2,1}^Q & q_{2,2}w_{2,2}^Q & \\dots & q_{2,d_{model}}w_{2,d_{model}}^Q \\\\\n",
        "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "    q_{\\text{seq},1}w_{\\text{seq},1}^Q & q_{\\text{seq},2}w_{\\text{seq},2}^Q & \\dots & q_{\\text{seq},d_{model}}w_{\\text{seq},d_{model}}^Q \\\\\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "    q_{1,1}' & q_{1,2}' & \\dots & q_{1,d_{model}}' \\\\\n",
        "    q_{2,1}' & q_{2,2}' & \\dots & q_{2,d_{model}}' \\\\\n",
        "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "    q_{\\text{seq},1}' & q_{\\text{seq},2}' & \\dots & q_{\\text{seq},d_{model}}' \\\\\n",
        "\\end{bmatrix}$,\n",
        "\n",
        "$K = K_{original}W_K = \\begin{bmatrix}\n",
        "    k_{1,1}' & k_{1,2}' & \\dots & k_{1,d_{model}}' \\\\\n",
        "    k_{2,1}' & k_{2,2}' & \\dots & k_{2,d_{model}}' \\\\\n",
        "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "    k_{\\text{seq},1}' & k_{\\text{seq},2}' & \\dots & k_{\\text{seq},d_{model}}' \\\\\n",
        "\\end{bmatrix}$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c5z982cEY_5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Split heads\n",
        "(batch size, seq length, $d_{model}$) --> (batch size, # heads, seq length, $d_k$)\n",
        "\n",
        "$\\begin{bmatrix}\n",
        "    q_{1,1}' & q_{1,2}' & \\dots & q_{1,d_{model}}' \\\\\n",
        "    q_{2,1}' & q_{2,2}' & \\dots & q_{2,d_{model}}' \\\\\n",
        "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "    q_{\\text{seq},1}' & q_{\\text{seq},2}' & \\dots & q_{\\text{seq},d_{model}}' \\\\\n",
        "\\end{bmatrix}$ --> $\\begin{bmatrix}\n",
        "    q_{1,1}' & \\dots & q_{1,k}'  \\\\\n",
        "    q_{2,1}' & \\dots &q_{2,k}'\\\\\n",
        "    \\vdots & \\ddots & \\vdots   \\\\\n",
        "    q_{\\text{seq},1}' & \\dots & q_{\\text{seq},k}' \\\\\n",
        "\\end{bmatrix}$ $\\begin{bmatrix}\n",
        "    q_{1,k+1}' & \\dots & q_{1,2k}'  \\\\\n",
        "    q_{2,k+1}' & \\dots &q_{2,2k}'\\\\\n",
        "    \\vdots & \\ddots & \\vdots   \\\\\n",
        "    q_{\\text{seq},k+1}' & \\dots & q_{\\text{seq},2k}' \\\\\n",
        "\\end{bmatrix}$ ...\n",
        "\n"
      ],
      "metadata": {
        "id": "oEUWpVdojKMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention calculation\n",
        "\n",
        "$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_{k}}}\\right) V$\n",
        "- **Q**: Why scaled? **A**: dot product $q.k = \\sum_{i=1}^{d_k} q_ik_i$. Assume $q$ and $k$ are independent, zero mean and unit variance. $E\\{q.k\\} = 0$, $Var(q.k) = d_k$. If dot products gets larger, it will enter the saturation region of softmax --> vanishing gradient\n",
        "- Attention has shape: (batch size, # heads, seq length,  $d_k$ )"
      ],
      "metadata": {
        "id": "es_wHcPfjPlE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combine heads\n",
        "(batch size, # heads, seq length,  dk) --> (batch size, seq length, $d_{model}$)"
      ],
      "metadata": {
        "id": "HrgldLUwlu75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\" # d_k = d_v = d_model/num_heads\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.d_k = d_model // num_heads\n",
        "\n",
        "    self.W_q = nn.Linear(d_model, d_model)\n",
        "    self.W_k = nn.Linear(d_model, d_model)\n",
        "    self.W_v = nn.Linear(d_model, d_model)\n",
        "    self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "  def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "\n",
        "  def split_heads(self, x):\n",
        "      batch_size, seq_length, d_model = x.size()\n",
        "      return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "  def combine_heads(self, x):\n",
        "      batch_size, _, seq_length, d_k = x.size()\n",
        "      return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "  def forward(self, Q, K, V, mask=None):\n",
        "      Q = self.split_heads(self.W_q(Q))\n",
        "      K = self.split_heads(self.W_k(K))\n",
        "      V = self.split_heads(self.W_v(V))\n",
        "\n",
        "      attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "      output = self.W_o(self.combine_heads(attn_output))\n",
        "      return output"
      ],
      "metadata": {
        "id": "kql364K-SAmf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Position-wise Feed-Forward Networks\n",
        "![image](https://github.com/guyuxuan9/UROP_robotic_arm/assets/58468284/4c2f54fb-27d0-4e19-a99f-0562123240d7)\n"
      ],
      "metadata": {
        "id": "vrHPgIkApIZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "fRtMzS7Ls1J-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Layer\n",
        "\n",
        "![image](https://github.com/guyuxuan9/Transformer-from-scratch/assets/58468284/28b7d53f-2d51-40db-9853-76a6ce400e7b)\n",
        "\n",
        "**Batch Normalisation vs. Layer Normalisation** (Why layer norm?)\n",
        "\n",
        "![image](https://github.com/guyuxuan9/Transformer-from-scratch/assets/58468284/5099cb58-16cf-48dd-be40-b79fc263c56e)\n",
        "\n",
        "- Layer Norm: normalise across all features of each input word\n",
        "- Batch Norm: normalise across batch of each feature\n",
        "\n",
        "The problem of Batch Norm in NLP is that the input sentence might have various length, which is indicated by figure below.\n",
        "\n",
        "![image](https://github.com/guyuxuan9/Transformer-from-scratch/assets/58468284/c6cd143c-bede-4a43-8c1a-0fb7781cf86b)\n",
        "\n",
        "If the sequence length is less than the max length, zero paddings will be used to fill in the empty positions. However, the zeros added will change the mean and variance of the batch (batch statistics). Therefore, batch norm is not used in NLP tasks."
      ],
      "metadata": {
        "id": "BeALJpTqXFEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "1d7ibT3BXdhk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder Layer\n",
        "\n",
        "![image](https://github.com/guyuxuan9/Transformer-from-scratch/assets/58468284/89dd4b3e-cdf7-4eb6-be9e-69d7af4a826f)"
      ],
      "metadata": {
        "id": "GmEisRYBfnuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        attn_output = self.self_attn(x, x, x, tgt_mask) # Q,K,V\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "UQ-GuPz3fxS5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer\n",
        "\n",
        "![image](https://github.com/guyuxuan9/Transformer-from-scratch/assets/58468284/0ec7bb55-5685-45ab-b87a-dbd269e7f3bf)\n",
        "\n",
        "- **Nx** in encoder and decoder means there are several layers. The output of the previous layer is fed to the input of the current layer.\n",
        "- **Purpose of the mask**: in output prediction, we don't want to look ahead, i.e. predict only based on the previous input\n",
        "\n",
        "![image](https://github.com/guyuxuan9/Transformer-from-scratch/assets/58468284/20259fc2-ccf9-4104-bda0-5be29a9a96e3)\n"
      ],
      "metadata": {
        "id": "iZUCHPvKhGfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2) # for the non-zero elements\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
        "        seq_length = tgt.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool() # 1 - upper triangular ones --> lower triangular ones\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        output = self.fc(dec_output)\n",
        "        output = F.softmax(output, dim=-1)\n",
        "        return output"
      ],
      "metadata": {
        "id": "UGT_dUgIhN05"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training & Testing"
      ],
      "metadata": {
        "id": "Fzh49OxADU9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size = 5000\n",
        "tgt_vocab_size = 5000\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "max_seq_length = 100\n",
        "dropout = 0.1\n",
        "\n",
        "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
        "\n",
        "# Generate random sample data\n",
        "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size=64, seq_length)\n",
        "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size=64, seq_length)"
      ],
      "metadata": {
        "id": "376encNqDWoU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qjpfA_CEWjr",
        "outputId": "1b916b85-1739-47b7-c622-b10afb592317"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1133, 3281, 1928,  ..., 3837, 3115,  781],\n",
              "        [4246,  473, 2231,  ...,  596, 4612, 3928],\n",
              "        [2661, 2564, 4253,  ..., 2675, 1316, 2385],\n",
              "        ...,\n",
              "        [2023, 4028, 4269,  ..., 2424,  640, 2002],\n",
              "        [4030,  749, 2769,  ..., 3078, 1955,  867],\n",
              "        [3401, 1888,  822,  ..., 3024,  379, 3776]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "transformer.train()\n",
        "\n",
        "for epoch in range(2):\n",
        "    optimizer.zero_grad()\n",
        "    output = transformer(src_data, tgt_data[:, :-1])\n",
        "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTkDeCYbDoZI",
        "outputId": "0f2013b0-fc54-4b9b-8149-80770271796f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 8.28443717956543\n",
            "Epoch: 2, Loss: 8.30500316619873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src = torch.tensor([[0, 2, 5, 6, 4, 3, 9, 5, 2, 9, 10, 1]])\n",
        "trg = torch.tensor([[0]])\n",
        "print(src.shape,trg.shape)\n",
        "out = transformer(src, trg)\n",
        "print(out, out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYQc-fCtE3LZ",
        "outputId": "0d58ffc4-d5ed-41b8-becd-52cf77bc7685"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 12]) torch.Size([1, 1])\n",
            "tensor([[[-0.1130, -0.2085, -0.2666,  ...,  0.2334,  0.9185, -0.3312]]],\n",
            "       grad_fn=<ViewBackward0>) torch.Size([1, 1, 5000])\n"
          ]
        }
      ]
    }
  ]
}